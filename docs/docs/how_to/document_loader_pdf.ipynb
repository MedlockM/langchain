{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# How to load PDFs\n",
    "\n",
    "[Portable Document Format (PDF)](https://en.wikipedia.org/wiki/PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.\n",
    "\n",
    "This guide covers how to [load](/docs/concepts/document_loaders/) `PDF` documents into the LangChain [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) format that we use downstream.\n",
    "\n",
    "Text in PDFs is typically represented via text boxes. They may also contain images. A PDF parser might do some combination of the following:\n",
    "\n",
    "- Agglomerate text boxes into lines, paragraphs, and other structures via heuristics or ML inference;\n",
    "- Run [OCR](https://en.wikipedia.org/wiki/Optical_character_recognition) on images to detect text therein;\n",
    "- Classify text as belonging to paragraphs, lists, tables, or other structures;\n",
    "- Structure text into table rows and columns, or key-value pairs.\n",
    "- Use multimodal LLM to convert the PDF as an image and describe its text and embedded images, page by page\n",
    "\n",
    "LangChain integrates with a host of PDF parsers. Some are simple and relatively low-level; others will support OCR and image-processing, or perform advanced document layout analysis. The right choice will depend on your needs. Below we enumerate the possibilities.\n",
    "\n",
    "We will demonstrate these approaches on a [sample file](https://github.com/langchain-ai/langchain/blob/master/libs/community/tests/integration_tests/examples/layout-parser-paper.pdf):"
   ],
   "id": "7d38063aef9243d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "file_path = (\n",
    "    \"../../docs/integrations/document_loaders/example_data/layout-parser-paper.pdf\"\n",
    ")"
   ],
   "id": "81428323cb859970"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    ":::info A note on multimodal models\n",
    "\n",
    "Many modern LLMs support inference over multimodal inputs (e.g., images). In some applications -- such as question-answering over PDFs with complex layouts, diagrams, or scans -- it may be advantageous to skip the PDF parsing, instead passing it to a vision model directly. We demonstrate an example of this in the [Use of multimodal models](/docs/how_to/document_loader_pdf/#use-of-multimodal-models) section below.\n",
    "\n",
    ":::\n",
    "\n",
    "## Simple and fast text extraction\n",
    "\n",
    "If you are looking for a simple string representation of text that is embedded in a PDF, the method below is appropriate. It will return a list of [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) objects-- one per page-- containing a single string of the page's text in the Document's `page_content` attribute. It will not parse text in images, tables or scanned PDF pages. Under the hood it uses the [pypdf](https://pypdf.readthedocs.io/en/stable/) Python library.\n",
    "\n",
    "LangChain [document loaders](/docs/concepts/document_loaders) implement `lazy_load` and its async variant, `alazy_load`, which return iterators of `Document` objects. We will use these below."
   ],
   "id": "a4886db0b2a43faa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%pip install -qU langchain_community pypdf",
   "id": "933ef65875ced742"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = []\n",
    "async for page in loader.alazy_load():\n",
    "    pages.append(page)"
   ],
   "id": "bc8049834c787c85"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pprint(pages[0].metadata)\n",
    "print(pages[0].page_content)"
   ],
   "id": "2df07021afe703ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Note that the metadata of each document stores the corresponding page number.\n",
    "\n",
    "### Vector search over PDFs\n",
    "\n",
    "Once we have loaded PDFs into LangChain `Document` objects, we can index them (e.g., a RAG application) in the usual way. Below we use OpenAI embeddings, although any LangChain [embeddings](https://python.langchain.com/docs/concepts/embedding_models) model will suffice."
   ],
   "id": "a730bb994ec10c47"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%pip install -qU langchain-openai",
   "id": "4c4ac12da253582f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
   ],
   "id": "3f0528ca1f16b8bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vector_store = InMemoryVectorStore.from_documents(pages, OpenAIEmbeddings())\n",
    "docs = vector_store.similarity_search(\"What is LayoutParser?\", k=2)\n",
    "for doc in docs:\n",
    "    print(f'Page {doc.metadata[\"page\"]}: {doc.page_content[:300]}\\n')"
   ],
   "id": "ee5d0d9b17d36eef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Extract and analyse images\n",
    "\n"
   ],
   "id": "77c764347a7885e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%pip install -qU rapidocr-onnxruntime",
   "id": "dc6ee800e0732542"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_community.document_loaders.parsers.images import (\n",
    "    RapidOCRBlobParser,\n",
    ")\n",
    "\n",
    "loader = PyPDFLoader(\n",
    "    file_path,\n",
    "    mode=\"page\",\n",
    "    extract_images=True,\n",
    "    images_parser=RapidOCRBlobParser(),\n",
    "    images_inner_format='markdown-img',\n",
    ")\n",
    "docs = loader.load()\n",
    "print(docs[5].page_content)"
   ],
   "id": "143a72310d651486"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "It is possible to ask a multimodal LLM to describe the image.",
   "id": "e0b1191c307f55b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"OpenAI API key =\")"
   ],
   "id": "aac6781143d775d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_community.document_loaders.parsers.images import LLMImageBlobParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "loader = PyPDFLoader(\n",
    "    file_path,\n",
    "    mode=\"page\",\n",
    "    extract_images=True,\n",
    "    images_parser=LLMImageBlobParser(model=llm),\n",
    "    images_inner_format=\"text\",\n",
    "    )\n",
    "docs = loader.load()\n",
    "print(docs[5].page_content)"
   ],
   "id": "208f75b767c5f919"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Extract tables",
   "id": "95e29498e2007554"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%pip install -qU PyMuPDF",
   "id": "647c63c65e26e634"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "loader = PyMuPDFLoader(\n",
    "    file_path,\n",
    "    mode=\"page\",\n",
    "    extract_tables=\"markdown\",\n",
    ")\n",
    "docs = loader.load()\n",
    "print(docs[4].page_content)"
   ],
   "id": "eda1fb51d8b2b301"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Layout analysis and extraction of text from images\n",
    "\n",
    "If you require a more granular segmentation of text (e.g., into distinct paragraphs, titles, tables, or other structures) or require extraction of text from images, the method below is appropriate. It will return a list of [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) objects, where each object represents a structure on the page. The Document's metadata stores the page number and other information related to the object (e.g., it might store table rows and columns in the case of a table object).\n",
    "\n",
    "Under the hood it uses the `langchain-unstructured` library. See the [integration docs](/docs/integrations/document_loaders/unstructured_file/) for more information about using [Unstructured](https://docs.unstructured.io/welcome) with LangChain.\n",
    "\n",
    "Unstructured supports multiple parameters for PDF parsing:\n",
    "- `strategy` (e.g., `\"auto\"`, `\"fast\"`, `\"ocr_only\"` or `\"hi_res\"`)\n",
    "- API or local processing. You will need an API key to use the API.\n",
    "\n",
    "The [hi-res](https://docs.unstructured.io/api-reference/how-to/choose-hi-res-model) strategy provides support for document layout analysis and OCR. We demonstrate it below via the API. See [local parsing](/docs/how_to/document_loader_pdf/#local-parsing) section below for considerations when running locally."
   ],
   "id": "82d1bd11f4759d60"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%pip install -qU langchain-unstructured",
   "id": "bdee83d46bebfef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"UNSTRUCTURED_API_KEY\" not in os.environ:\n",
    "    os.environ[\"UNSTRUCTURED_API_KEY\"] = getpass.getpass(\"Unstructured API Key:\")"
   ],
   "id": "3231cfe505edef11"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As before, we initialize a loader and load documents lazily:",
   "id": "363a85b85600930e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_unstructured import UnstructuredLoader\n",
    "\n",
    "loader = UnstructuredLoader(\n",
    "    file_path=file_path,\n",
    "    strategy=\"hi_res\",\n",
    "    partition_via_api=True,\n",
    "    coordinates=True,\n",
    ")\n",
    "docs = []\n",
    "for doc in loader.lazy_load():\n",
    "    docs.append(doc)"
   ],
   "id": "25dc250aa3db5b71"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here we recover 187 distinct structures over the 16 page document:",
   "id": "df95eef31c282b25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(len(docs))",
   "id": "65910b3de8640af7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can use the document metadata to recover content from a single page:",
   "id": "e8543a29ebda976c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "first_page_docs = [doc for doc in docs if doc.metadata.get(\"page_number\") == 0]\n",
    "\n",
    "for doc in first_page_docs:\n",
    "    print(doc.page_content)"
   ],
   "id": "a254480bfbacd86b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Extracting tables and other structures\n",
    "\n",
    "Each `Document` we load represents a structure, like a title, paragraph, or table.\n",
    "\n",
    "Some structures may be of special interest for indexing or question-answering tasks. These structures may be:\n",
    "1. Classified for easy identification;\n",
    "2. Parsed into a more structured representation.\n",
    "\n",
    "Below, we identify and extract a table:"
   ],
   "id": "77a9a5cb4bc56ce6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<details>\n",
    "<summary>Click to expand code for rendering pages</summary>"
   ],
   "id": "ceac667f6a0a405a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%pip install -qU matplotlib PyMuPDF pillow",
   "id": "401ccc2d67c75bb5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import fitz\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def plot_pdf_with_boxes(pdf_page, segments):\n",
    "    pix = pdf_page.get_pixmap()\n",
    "    pil_image = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "\n",
    "    fig, ax = plt.subplots(1, figsize=(10, 10))\n",
    "    ax.imshow(pil_image)\n",
    "    categories = set()\n",
    "    category_to_color = {\n",
    "        \"Title\": \"orchid\",\n",
    "        \"Image\": \"forestgreen\",\n",
    "        \"Table\": \"tomato\",\n",
    "    }\n",
    "    for segment in segments:\n",
    "        points = segment[\"coordinates\"][\"points\"]\n",
    "        layout_width = segment[\"coordinates\"][\"layout_width\"]\n",
    "        layout_height = segment[\"coordinates\"][\"layout_height\"]\n",
    "        scaled_points = [\n",
    "            (x * pix.width / layout_width, y * pix.height / layout_height)\n",
    "            for x, y in points\n",
    "        ]\n",
    "        box_color = category_to_color.get(segment[\"category\"], \"deepskyblue\")\n",
    "        categories.add(segment[\"category\"])\n",
    "        rect = patches.Polygon(\n",
    "            scaled_points, linewidth=1, edgecolor=box_color, facecolor=\"none\"\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    # Make legend\n",
    "    legend_handles = [patches.Patch(color=\"deepskyblue\", label=\"Text\")]\n",
    "    for category in [\"Title\", \"Image\", \"Table\"]:\n",
    "        if category in categories:\n",
    "            legend_handles.append(\n",
    "                patches.Patch(color=category_to_color[category], label=category)\n",
    "            )\n",
    "    ax.axis(\"off\")\n",
    "    ax.legend(handles=legend_handles, loc=\"upper right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def render_page(doc_list: list, page_number: int, print_text=True) -> None:\n",
    "    pdf_page = fitz.open(file_path).load_page(page_number - 1)\n",
    "    page_docs = [\n",
    "        doc for doc in doc_list if doc.metadata.get(\"page_number\") == page_number\n",
    "    ]\n",
    "    segments = [doc.metadata for doc in page_docs]\n",
    "    plot_pdf_with_boxes(pdf_page, segments)\n",
    "    if print_text:\n",
    "        for doc in page_docs:\n",
    "            print(f\"{doc.page_content}\\n\")"
   ],
   "id": "6242fdee50786eaa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "</details>",
   "id": "ebcc3b9fdade316f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "render_page(docs, 5)",
   "id": "74a439797808e60f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Note that although the table text is collapsed into a single string in the document's content, the metadata contains a representation of its rows and columns:",
   "id": "a31dbd783ccc2af8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "segments = [\n",
    "    doc.metadata\n",
    "    for doc in docs\n",
    "    if doc.metadata.get(\"page_number\") == 5 and doc.metadata.get(\"category\") == \"Table\"\n",
    "]\n",
    "\n",
    "display(HTML(segments[0][\"text_as_html\"]))"
   ],
   "id": "f57be1437676be16"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<table><thead><tr><th colspan=\"3\">able 1. LUllclll 1ayoul actCCLloll 1110AdCs 111 L1C LayoOulralsel 1110U4cl 200</th></tr><tr><th>Dataset</th><th>| Base Model\\'|</th><th>Notes</th></tr></thead><tbody><tr><td>PubLayNet [38]</td><td>F/M</td><td>Layouts of modern scientific documents</td></tr><tr><td>PRImA</td><td>M</td><td>Layouts of scanned modern magazines and scientific reports</td></tr><tr><td>Newspaper</td><td>F</td><td>Layouts of scanned US newspapers from the 20th century</td></tr><tr><td>TableBank [18]</td><td>F</td><td>Table region on modern scientific and business document</td></tr><tr><td>HJDataset</td><td>F/M</td><td>Layouts of history Japanese documents</td></tr></tbody></table>",
   "id": "abe284b998ef27f9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Extracting text from specific sections\n",
    "\n",
    "Structures may have parent-child relationships -- for example, a paragraph might belong to a section with a title. If a section is of particular interest (e.g., for indexing) we can isolate the corresponding `Document` objects.\n",
    "\n",
    "Below, we extract all text associated with the document's \"Conclusion\" section:"
   ],
   "id": "767efcfbdc2e02c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "render_page(docs, 14, print_text=False)",
   "id": "b52c739e73b7ba14"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "conclusion_docs = []\n",
    "parent_id = -1\n",
    "for doc in docs:\n",
    "    if doc.metadata[\"category\"] == \"Title\" and \"Conclusion\" in doc.page_content:\n",
    "        parent_id = doc.metadata[\"element_id\"]\n",
    "    if doc.metadata.get(\"parent_id\") == parent_id:\n",
    "        conclusion_docs.append(doc)\n",
    "\n",
    "for doc in conclusion_docs:\n",
    "    print(doc.page_content)"
   ],
   "id": "9b156d4de38fff12"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Extracting text from images\n",
    "\n",
    "OCR is run on images, enabling the extraction of text therein:"
   ],
   "id": "1f5f03a76eb0b470"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "render_page(docs, 11)",
   "id": "90cbd7412cea15b1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Note that the text from the figure on the right is extracted and incorporated into the content of the `Document`.",
   "id": "8098fff61ee52aa8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Local parsing\n",
    "\n",
    "Parsing locally requires the installation of additional dependencies.\n",
    "\n",
    "**Poppler** (PDF analysis)\n",
    "- Linux: `apt-get install poppler-utils`\n",
    "- Mac: `brew install poppler`\n",
    "- Windows: https://github.com/oschwartz10612/poppler-windows\n",
    "\n",
    "**Tesseract** (OCR)\n",
    "- Linux: `apt-get install tesseract-ocr`\n",
    "- Mac: `brew install tesseract`\n",
    "- Windows: https://github.com/UB-Mannheim/tesseract/wiki#tesseract-installer-for-windows\n",
    "\n",
    "We will also need to install the `unstructured` PDF extras:"
   ],
   "id": "deda1e00ce825874"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%pip install -qU \"unstructured[pdf]\"",
   "id": "b0d1f294679a1cab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can then use the [UnstructuredLoader](https://python.langchain.com/api_reference/unstructured/document_loaders/langchain_unstructured.document_loaders.UnstructuredLoader.html) much the same way, forgoing the API key and `partition_via_api` setting:",
   "id": "15ef8278b3b25c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "loader_local = UnstructuredLoader(\n",
    "    file_path=file_path,\n",
    "    strategy=\"hi_res\",\n",
    ")\n",
    "docs_local = []\n",
    "for doc in loader_local.lazy_load():\n",
    "    docs_local.append(doc)"
   ],
   "id": "275c8a0320917def"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The list of documents can then be processed similarly to those obtained from the API.\n",
    "\n",
    "## Use of multimodal models - ZeroxPDFLoader\n",
    "\n",
    "Many modern LLMs support inference over multimodal inputs (e.g., images). In some applications-- such as question-answering over PDFs with complex layouts, diagrams, or scans-- it may be advantageous to skip the PDF parsing, instead passing it to a vision model directly using `ZeroxPDFLoader`. This allows a model to reason over the two dimensional content on the page, instead of a \"one-dimensional\" string representation.\n",
    "\n",
    "A list of the compatible models can be found in the [Zerox documentation](https://github.com/getomni-ai/zerox). Below we use OpenAI's `gpt-4o-mini`.\n"
   ],
   "id": "aabd7c162f385e77"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%pip install -qU langchain_community zerox",
   "id": "40476c76f56cd1ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# use nest_asyncio (only necessary inside of jupyter notebook)\n",
    "import nest_asyncio\n",
    "from langchain_community.document_loaders.pdf import ZeroxPDFLoader\n",
    "\n",
    "nest_asyncio.apply()\n",
    "# Initialize ZeroxPDFLoader with the desired model\n",
    "loader = ZeroxPDFLoader(file_path=file_path, model=\"gpt-4o-mini\")\n",
    "docs = loader.load()\n",
    "print(docs[0].page_content)"
   ],
   "id": "8d00924790e6a4ab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Other PDF loaders\n",
    "\n",
    "For a list of available LangChain PDF loaders, please see [this table](/docs/integrations/document_loaders/#pdfs)."
   ],
   "id": "7491d93e39a189da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "929bfc34f3e94528"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patch-langchain",
   "language": "python",
   "name": "patch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
